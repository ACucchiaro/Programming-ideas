{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_gm3mDtniru"
      },
      "source": [
        "#REGRESIÓN LINEAL SIMPLE\n",
        "from sklearn import linear_model\n",
        "reg = linear_model.LinearRegression() #Se crea un objeto para la regresón lineal\n",
        "reg.fit(df[['x']],df.y) #Se entrena el modelo de regresión lineal\n",
        "reg.predict() #Para predecir un par de valores\n",
        "reg.coef_ #Valor de la pendiente\n",
        "reg.intercept_ #Valor de la ordenada al origen\n",
        "\n",
        "%matplotlib inline #Para graficar la regresión\n",
        "plt.xlabel('x', fontsize=20)\n",
        "plt.ylabel('y',fontsize=20)\n",
        "plt.scatter(df.x,df.y,color='red',marker='+')\n",
        "plt.plot(df.x,reg.predict(df[['x']]),color='blue')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IRCUJavogC7"
      },
      "source": [
        "#REGRESIÓN LINEAL MÚLTIPLE\n",
        "from sklearn import linear_model\n",
        "reg = linear_model.LinearRegression() #Se crea un objeto para la regresión lineal\n",
        "reg.fit(df[['x1','x2','x3']],df.y) #Se entrena el modelo de regresión lineal\n",
        "reg.coef_ #Valor de los parámetros\n",
        "reg.intercept_ #Valor de la ordenada al origen\n",
        "reg.predict([[x1,x2,x3]]) #Para predecir un valor de y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WM-JVsJrdTY"
      },
      "source": [
        "#VARIABLES DUMMY\n",
        "dummies = pd.get_dummies(df.columna_categórica) #Matriz esparza\n",
        "merged = pd.concat([df,dummies],axis='columns') #Uno los dataframes\n",
        "final = merged.drop(['columna_categórica','alguna_columna_de_la_matriz_esparza'],axis='columns')\n",
        "from sklearn.linear_model import LinearRegression\n",
        "model = LinearRegression()\n",
        "X = final.drop('y',axis='columns') #Se elimina la columna de la variable a predecir\n",
        "y = final.y\n",
        "model.fit(X,y)\n",
        "model.score(X,y) #Accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Dd-WR5N1itC"
      },
      "source": [
        "#ONE HOT ENCODER\n",
        "from sklearn.preprocessing impor LabelEncoder\n",
        "le = LabelEncoder()\n",
        "dfle = df\n",
        "dfle.columna_categórica = le.fit_transform(dfle.columna_categórica)\n",
        "X = dfle[['columna_categórica','x1']].values\n",
        "y = dfle.y\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ohe = OneHotEncoder(categorical_features=[0])\n",
        "X = ohe.fit_transform(X).toarray()\n",
        "X = X[:,1:]\n",
        "model.fit(X,y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwdmK7b-3f2r"
      },
      "source": [
        "#ENTRENAMIENTO Y TESTEO DE LA DATA\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=10) #20% para test y 80% para entrenamiento y con random_state no cambia\n",
        "from sklear.linear_model import LinearRegression\n",
        "clf = LinearRegression()\n",
        "clf.fit(X_train,y_train)\n",
        "clf.predict(X_test)\n",
        "clf.score(X_test,y_test) #Acurracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQRtlPne5TcK"
      },
      "source": [
        "#REGRESIÓN LOGÍSTICA --> PREDICCIÓN CATEGÓRICA\n",
        "#CLASIFICACIÓN BINARIA\n",
        "from sklearn.model_selecttion import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[['x1']],df.y,test_size=0.2)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train,y_train)\n",
        "model.predict(X_test)\n",
        "model.score(X_test,y_test) #Accuracy\n",
        "model.predict_proba(X_test) #Tira las probabilidades"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkWWf_kB-AY4"
      },
      "source": [
        "#REGRESIÓN LOGÍSTICA --> PREDICCIÓN CATEGÓRICA\n",
        "#CLASIFICACIÓN MULTICLASE\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[['x1','x2']],df.y, test_size=0.2)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "model.score(X_test, y_test) #Accuracy\n",
        "\n",
        "from sklearn.metrics impor confusion_matrix #Para ver donde estás los errores\n",
        "y_predicted = model.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_predicted)\n",
        "import seaborn as sn\n",
        "plt.figure(figsize = (10,7))\n",
        "sn.heatmap(cm,annot=True)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Truth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohMPrWm3CBRd"
      },
      "source": [
        "#ÁRBOLES DE DECISIÓN --> LABELENCODER PARA TRANSFORMAR VARIABLES CATEGÓRICAS EN NUMÉRICAS\n",
        "inputs = df.drop('columna_target',axis='columns')\n",
        "target = df['columna_target']\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le_x1 = LabelEncoder()\n",
        "le_x2 = LabelEncoder()\n",
        "le_x3 = LabelEncoder()\n",
        "inputs['nueva_columna1'] = le_x1.fit_transform(inputs['x1'])\n",
        "inputs['nueva_columna2'] = le_x2.fit_transform(inputs['x2'])\n",
        "inputs['nueva_columna3'] = le_x3.fit_transform(inputs['x3'])\n",
        "inputs_n = inputs.drop(['x1','x2','x3'], axis='columns')\n",
        "from sklearn import tree\n",
        "model = tree.DecisionTreeClassifier()\n",
        "model.fit(inputs_n,columna_target) #Se debería separar en train y test\n",
        "#El criterio puede ser gini o entropía, entre otros\n",
        "model.score(inputs_n, target) #El score es 1 porque se usa el mismo dataset que para el train\n",
        "model.predict([[]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13UmxB0rFOLd"
      },
      "source": [
        "#RANDOM FOREST\n",
        "from sklear.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.drop(['target'],axis='columns'),df.target,test_size=0.2)\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train) #n_estimators=N° de árboles\n",
        "model.score(X_test, y_test)\n",
        "y_predicted = model.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix #Para ver donde estás los errores\n",
        "y_predicted = model.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_predicted)\n",
        "import seaborn as sn\n",
        "plt.figure(figsize = (10,7))\n",
        "sn.heatmap(cm,annot=True)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Truth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nknBFV8WWw3a"
      },
      "source": [
        "#K fold cross validation\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_digits\n",
        "\n",
        "digits = load_digits()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(digits.data,digits.target,test_size=0.3)\n",
        "\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "lr.score(X_test,y_test)\n",
        "\n",
        "svm = SVC()\n",
        "svm.fit(X_train, y_train)\n",
        "svm.score(X_test, y_test)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=40)\n",
        "rf.fit(X_train, y_train)\n",
        "rf.score(X_test,y_test)\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "kf = KFold(n_splits=3)\n",
        "kf\n",
        "\n",
        "for train_index, test_index in kf.split([1,2,3,4,5,6,7,8,9]):\n",
        "  print(train_index, test_index)\n",
        "\n",
        "def get_score(model, X_train, X_test, y_train, y_test):\n",
        "  model.fit(X_train, y_train)\n",
        "  return model.score(X_test, y_test)\n",
        "\n",
        "get_score(LogisticRegression(), X_train, X_test, y_train, y_test)\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "folds = StratifiedKFold(n_splits=3)\n",
        "scores_1=[]\n",
        "scores_svm=[]\n",
        "scores_rf=[]\n",
        "\n",
        "for train_index, test_index in kf.split(digits.data):\n",
        "  X_train, X_test, y_train, y_test = digits.data[train_index], digits.data[test_index], digits.target[train_index], digits.target[test_index]\n",
        "  scores_1.append(get_score(LogisticRegression(),X_train,X_test,y_train,y_test))\n",
        "  scores_svm.append(get_score(SVC(),X_train,X_test,y_train,y_test))\n",
        "  scores_rf.append(get_score(RandomForestClassifier(n_estimators=40),X_train,X_test,y_train,y_test))\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "cross_val_score(LogisticRegression(), digits.data,digits.target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vL9cOPa8sqtK"
      },
      "source": [
        "#K Means Clustering\n",
        "from sklearn.cluster import KMeans\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.scatter(df['x1'],df['x2'])\n",
        "\n",
        "km = KMeans(n_clusters=3)\n",
        "y_predicted = km.fit_predict(df[['x1','x2']])\n",
        "\n",
        "df['cluster'] = y_predicted\n",
        "df1 = df[df.cluster == 0]\n",
        "df2 = df[df.cluster == 1]\n",
        "df2 = df[df.cluster == 2]\n",
        "plt.scatter(df1.x1,df1['x2'],color='green')\n",
        "plt.scatter(df2.x1,df2['x2'],color='red')\n",
        "plt.scatter(df3.x1,df3['x2'],color='black')\n",
        "plt.xlabel('x1')\n",
        "plt.ylabel('x2')\n",
        "plt.legend()\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(df[['x2']])\n",
        "df['x2'] = scaler.transform(df['x2'])\n",
        "scaler.fit(df.x1)\n",
        "df.x1 = scaler.transform(df.x1)\n",
        "\n",
        "km = KMeans(n_clusters=3)\n",
        "y_predicted = km.fit_predict(df[['x1','x2']])\n",
        "df['cluster'] = y_predicted\n",
        "df1 = df[df.cluster == 0]\n",
        "df2 = df[df.cluster == 1]\n",
        "df2 = df[df.cluster == 2]\n",
        "plt.scatter(df1.x1,df1['x2'],color='green')\n",
        "plt.scatter(df2.x1,df2['x2'],color='red')\n",
        "plt.scatter(df3.x1,df3['x2'],color='black')\n",
        "plt.legend()\n",
        "\n",
        "km.cluster_centers_ #Centros\n",
        "plt.scatter(km.cluster_ceters_[:,0],km.cluster_centers_[:,1],color='purple',marker='*',label='centroid')\n",
        "\n",
        "k_rng = range(1,10) #Técnica del codo\n",
        "sse = []\n",
        "for k in k_rng:\n",
        "  km = KMeans(n_clusters=k)\n",
        "  km.fit(df[['x1','x2']])\n",
        "  sse.append(km.inertia_)\n",
        "plt.xlabel('K')\n",
        "plt.ylabel('Suma de errores cuadráticos')\n",
        "plt.plot(k_rng,sse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRqOdrl808ef"
      },
      "source": [
        "#PREPROCESAMIENTO\n",
        "df.columns[df.isna().any()] #Para ver si hay un NaN en el DataFrame\n",
        "#Si la columna que tiene valores Nan es \"columna\", para rellenar el espacio con la media:\n",
        "df.columna = df.columna.fillna(df.columna.mean())\n",
        "\n",
        "df.isnull().sum() #Te dice la cantidad de valores no disponibles\n",
        "df.dropna() #Elimina todos los valores no disponibles, lo cual es recomendable si hay mucha data y los valores inexistentes son pocos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdDyEWhO6A4A"
      },
      "source": [
        "#NAIVE BAYES\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df,target,test_size=0.2)\n",
        "model = GaussianNB()\n",
        "model.fit(X_train,y_train)\n",
        "model.score(X_test,y_test)\n",
        "model.predict(X_test[:10])\n",
        "model.predict_proba(X_test[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYeyUBYV8_RI"
      },
      "source": [
        "#HIPERPARÁMETROS\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "clf = GridSearchCV(svm.SVC(gamma='auto'),{\n",
        "    'C':[1,10,20],\n",
        "    'kernel':['rbf','linear']\n",
        "}, cv=5, return_train_score=False)\n",
        "clf.fit(df.data,df.target)\n",
        "clf.cv_results_\n",
        "df_results = pd.DataFrame(clf.cv_results_)[['param_C','param_kernel','mean_test_score']]\n",
        "clf.best_score_ #Mejor score\n",
        "clf.best_params_ #Mejores parámetros\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "rs = RandomizedSearchCV(svm.SVC(gamma='auto'),{\n",
        "    'C':[1,10,20]\n",
        "    'kernel':['rbf','linear']\n",
        "}, cv=5, return_train_scores=False,n_iter=2)\n",
        "rs.fit(df.data,df.target)\n",
        "pd.DataFrame(rs.cv_results_)[['param_C','param_kernel','mean_test_score']]\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model_params = {\n",
        "    'svm':{\n",
        "        'model':svm.SVC(gamma='auto'),\n",
        "        'params':{\n",
        "            'C':[1,10,20],\n",
        "            'kernel':['rbf','linear']\n",
        "        }\n",
        "    },\n",
        "    'random_forest':{\n",
        "        'model':RandomForestClassifier(),\n",
        "        'params':{\n",
        "            'n_estimators':[1,5,10]\n",
        "        }\n",
        "    },\n",
        "    'logistic_regression':{\n",
        "        'model':LogisticRegression(solver='liblinear',multi_class='auto'),\n",
        "        'params':{\n",
        "            'C':[1,5,10]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "scores =[]\n",
        "for model_name, mp in model_params.items():\n",
        "  clf = GridSearchCV(mp['model'],mp['params'],cv=5,return_train_score=False)\n",
        "  clf.fit(df.data,df.target)\n",
        "  scores.append({\n",
        "      'model': model_name,\n",
        "      'best_score': clf.best_score_,\n",
        "      'best_params': clf.best_params_\n",
        "  })\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}